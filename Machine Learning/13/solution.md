Decision Tree is both a classification and a regression model. The decision tree algorithm essentially functions on ‘if this, then what’ condition. It answers sequential questions that send us down the certain route and the depth of the tree. Hence, the depth of the tree is a very important parameter. For example, sometimes you may get the answer to the question by decision tree method, in say 3 steps while for certain conditions, you may reach the answer to your question in 4 steps. This is where the depth factor comes into play. Nevertheless, the decision tree is a widely used ML algorithm because it is fast enough, can handle both numerical and categorical data while performing extremely well on large datasets and being easy to visualize. However, they suffer from the problem of overfitting.

On the other hand, Random Forest is basically a combination of many decision trees. It is a collection of a large number of decision trees whose results are simply aggregated into one. They have the ability to limit overfitting without increasing the bias. It works in a way like we have 10 features in our dataset. So, the random forest will randomly select, let us say 3 features out of 10 and make a decision tree on the basis of the selected feature. But here, 7 features have been eliminated which may be very important ones for the decision-making process. As said earlier, they are a combination of many different decision trees, hence nearly all of the features are being utilized because every time it is selecting 3 random features. This is the reason why the error is less and they are less likely to be overfitted than a decision tree. They are more robust than a single decision tree.

Again, choice of algorithm is absolutely yours. The model, which gives better accuracy for the problem is always good choice.

